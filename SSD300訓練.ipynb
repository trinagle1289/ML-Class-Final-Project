{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0223fdb5-0351-4d3f-9db3-aaefc88329c8",
   "metadata": {},
   "source": [
    "#### 模型路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71de4283-a9ed-4a1c-937f-bb725665d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='file_name.pth' # 副檔名通常以.pt或.pth儲存，建議使用.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1330e-7891-416e-9cfd-eb239624b70e",
   "metadata": {},
   "source": [
    "#### 參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ab26d6-dde9-4296-a7ff-0d3c139369d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "device=torch.device('cuda') # 'cuda'/'cpu'，import torch\n",
    "num_classes=6 # 物件類別數+1(背景)\n",
    "train_size=523\n",
    "valid_size=0\n",
    "batch_size=1\n",
    "learning_rate=0.0001\n",
    "step_size=500 # Reriod of learning rate decay\n",
    "threshold=0.5 # 錨框匹配為物件/背景的閥值，參考值=0.5\n",
    "variances=[0.1,0.2] # 設定gHat中cx、cy與w、h間的權重\n",
    "alpha=1 # multi-task loss function內的權重\n",
    "epochs=200\n",
    "TrainingImage=r'./resources/training_image/'\n",
    "Annotation=r'./resources/annotation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06de3232-e00e-4406-ad61-97fb7965aaa1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transforms=transforms.Compose([transforms.Resize((300,300)),transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]) # ToTensor將影像像素歸一化至0~1(直接除以255)，from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb822b37-86eb-47d4-933d-4e59ae8c2c28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 建立dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaa7ca92-2cb3-4e18-bbd4-d2169f686c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class ImageLabel(Dataset): # from torch.utils.data import Dataset\n",
    "    def __init__(self,img,bbox,cls):\n",
    "        self.img=img\n",
    "        self.bbox=bbox\n",
    "        self.cls=cls\n",
    "    def __getitem__(self,idx):\n",
    "        return self.img[idx],self.bbox[idx],self.cls[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "def collate_fn(batch):\n",
    "    img=list()\n",
    "    bbox=list()\n",
    "    cls=list()\n",
    "    for data in batch:\n",
    "        img.append(data[0])\n",
    "        bbox.append(data[1])\n",
    "        cls.append(data[2])\n",
    "    img=torch.stack(img,dim=0) # import torch\n",
    "    return img,bbox,cls\n",
    "import os\n",
    "all_image_name=os.listdir(TrainingImage) # 所有影像檔名(含.jpg)，import os\n",
    "img=list()\n",
    "bbox=list()\n",
    "cls=list()\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "for image_name in all_image_name:\n",
    "    chi_en=list()\n",
    "    chi_hua=list()\n",
    "    I=Image.open(TrainingImage+image_name,mode='r') # from PIL import Image\n",
    "    I=transforms(I)\n",
    "    img.append(I) # 列表長度為影像個數，列表中每個元素為一個[3,300,300]的tensor\n",
    "    image_name=image_name[:-4] # 移除4個字元(.jpg)\n",
    "    root=ET.parse(Annotation+image_name+'.xml').getroot() # 獲取xml文件物件的根結點，import xml.etree.ElementTree as ET\n",
    "    size=root.find('size') # 獲取size子結點\n",
    "    width=int(size.find('width').text) # 原始影像的寬(像素)\n",
    "    height=int(size.find('height').text) # 原始影像的高(像素)\n",
    "    width_scale=I.size(1)/width # 輸入影像與原始影像的寬比\n",
    "    height_scale=I.size(1)/height # 輸入影像與原始影像的高比\n",
    "    for object in root.iter('object'): # 遞迴查詢所有的object子結點\n",
    "        bndbox=object.find('bndbox')\n",
    "        chi_en.append([int(bndbox.find('xmin').text)*width_scale,int(bndbox.find('ymin').text)*height_scale,int(bndbox.find('xmax').text)*width_scale,int(bndbox.find('ymax').text)*height_scale])    \n",
    "        name=object.find('name').text # 1,2,3,4,...  \n",
    "        chi_hua.append(int(name))\n",
    "    chi_en=torch.Tensor(chi_en) # 將chi_en轉成tensor，[該影像中的物件個數,4]，import torch\n",
    "    chi_hua=torch.Tensor(chi_hua) # 將chi_hua轉成tensor，import torch\n",
    "    bbox.append(chi_en) # 列表長度為影像個數，列表中每個元素為一個[該影像中的物件個數,4]的tensor\n",
    "    cls.append(chi_hua) # 列表長度為影像個數，列表中每個元素為一個[該影像中的物件個數]的tensor\n",
    "dataset=ImageLabel(img,bbox,cls)\n",
    "train_data,valid_data=torch.utils.data.random_split(dataset,[train_size,valid_size]) # import torch\n",
    "train_loader=torch.utils.data.DataLoader(train_data,batch_size,shuffle=True,collate_fn=collate_fn) # imort torch\n",
    "valid_loader=torch.utils.data.DataLoader(valid_data,batch_size,shuffle=False,collate_fn=collate_fn) # imort torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a425adb-3516-4b0e-89b9-ea44964df2a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 建立錨框"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c7b544-933d-48f0-a842-9d0419674429",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scale=[38,19,10,5,3,1] # 預測用的特徵圖尺寸(以像素為單位)\n",
    "sk=[0.07,0.15,0.33,0.51,0.69,0.87,1.05] # 各預測特徵圖的默認框尺度(相對於輸入影像的比例)，比預測特徵圖的個數多1\n",
    "aspect_ratio=[[1,2,1/2],[1,2,3,1/2,1/3],[1,2,3,1/2,1/3],[1,2,3,1/2,1/3],[1,2,1/2],[1,2,1/2]] # 各預測特徵圖的縱橫比(須檢查loc、conf的濾波器個數)\n",
    "abox=[]\n",
    "import itertools\n",
    "import math\n",
    "for i,j in enumerate(feature_scale):\n",
    "    for m,n in itertools.product(range(j),repeat=2):\n",
    "        cx=(n+0.5)/j # 等同於cx相對於輸入影像的比例位置(乘以輸入影像尺寸即為cx在輸入影像的像素位置)\n",
    "        cy=(m+0.5)/j # 等同於cy相對於輸入影像的比例位置(乘以輸入影像尺寸即為cy在輸入影像的像素位置)\n",
    "        for ar in aspect_ratio[i]:\n",
    "            abox+=[cx-sk[i]*math.sqrt(ar)/2,cy-sk[i]/math.sqrt(ar)/2,cx+sk[i]*math.sqrt(ar)/2,cy+sk[i]/math.sqrt(ar)/2] # [cxmin cymin cxmax cymax]\n",
    "        abox+=[cx-math.sqrt(sk[i]*sk[i+1])/2,cy-math.sqrt(sk[i]*sk[i+1])/2,cx+math.sqrt(sk[i]*sk[i+1])/2,cy+math.sqrt(sk[i]*sk[i+1])/2] # [xmin ymin xmax ymax]\n",
    "anchor=torch.Tensor(abox).view(-1,4).to(device) # [8732,4] (所有錨框的[xmin ymin xmax ymax]，皆相對於輸入影像的比例位置，乘以輸入影像尺寸即為在輸入影像的像素位置)，import torch\n",
    "anchor.clamp_(max=1, min=0) # 限定最大值為1、最小值0\n",
    "anchor=anchor*300 # 轉換成模型輸入影像尺寸(300*300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6cbd2f-a160-49d5-ad53-13a54ddb8016",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 模型類別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb24ebe-e9c4-4afa-a48e-d3641bf09210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class SSD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # block_1：Conv1_1~Conv4_3+ReLU\n",
    "        self.block_1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3,stride=1,padding=1), # [batch_size,64,300,300]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1), # [batch_size,64,300,300]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # [batch_size,64,150,150]\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1), # [batch_size,128,150,150]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # [batch_size,128,150,150]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # [batch_size,128,75,75]\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1), # [batch_size,256,75,75]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,stride=1,padding=1), # [batch_size,256,75,75]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,stride=1,padding=1), # [batch_size,256,75,75]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2,ceil_mode=True), # [batch_size,256,38,38]\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,38,38]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,38,38] \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,38,38]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Layer learns to scale the L2 normalized features from conv4_3\n",
    "        self.l2norm=L2Norm(512,20) # 512為輸入的特徵圖個數，20為scale\n",
    "         \n",
    "        # block_2：Pool4~Conv7+ReLU\n",
    "        self.block_2=nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # [batch_size,512,19,19]\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=1,padding=1), # [batch_size,512,19,19]\n",
    "            nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3,stride=1,padding=6,dilation=6), # [batch_size,1024,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=1024,out_channels=1024,kernel_size=1,stride=1), # [batch_size,1024,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # block_3：Conv8_1~Conv8_2+ReLU\n",
    "        self.block_3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024,out_channels=256,kernel_size=1), # [batch_size,256,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,stride=2,padding=1), # [batch_size,512,10,10]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # block_4：Conv9_1~Conv9_2+ReLU\n",
    "        self.block_4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=128,kernel_size=1), # [batch_size,128,10,10]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=2,padding=1), # [batch_size,256,5,5]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # block_5：Conv10_1~Conv10_2+ReLU\n",
    "        self.block_5=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=128,kernel_size=1), # [batch_size,128,5,5]\n",
    "            nn.ReLU(inplace=True),                            \n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3), # [batch_size,256,3,3]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # block_6：Conv11_1~Conv11_2+ReLU\n",
    "        self.block_6=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=128,kernel_size=1), # [batch_size,128,3,3]                            \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3), # [batch_size,256,1,1]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # loc_1\n",
    "        self.loc_1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=4*4,kernel_size=3,stride=1,padding=1), # [batch_size,16,38,38]\n",
    "        )\n",
    "        # conf_1\n",
    "        self.conf_1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=4*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(4*num_classes),38,38]\n",
    "        )\n",
    "        # loc_2\n",
    "        self.loc_2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024,out_channels=6*4,kernel_size=3,stride=1,padding=1), # [batch_size,24,19,19]\n",
    "        )\n",
    "        # conf_2\n",
    "        self.conf_2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024,out_channels=6*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(6*num_classes),19,19]\n",
    "        ) \n",
    "        # loc_3\n",
    "        self.loc_3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=6*4,kernel_size=3,stride=1,padding=1), # [batch_size,24,10,10]\n",
    "        )\n",
    "        # conf_3\n",
    "        self.conf_3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=6*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(6*num_classes),10,10]\n",
    "        ) \n",
    "        # loc_4\n",
    "        self.loc_4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=6*4,kernel_size=3,stride=1,padding=1), # [batch_size,24,5,5]\n",
    "        )\n",
    "        # conf_4\n",
    "        self.conf_4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=6*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(6*num_classes),5,5]\n",
    "        )       \n",
    "        # loc_5\n",
    "        self.loc_5=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=4*4,kernel_size=3,stride=1,padding=1), # [batch_size,16,3,3]\n",
    "        )\n",
    "        # conf_5\n",
    "        self.conf_5=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=4*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(4*num_classes),3,3]\n",
    "        )   \n",
    "        # loc_6\n",
    "        self.loc_6=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=4*4,kernel_size=3,stride=1,padding=1), # [batch_size,16,1,1]\n",
    "        )\n",
    "        # conf_6\n",
    "        self.conf_6=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=4*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(4*num_classes),1,1]\n",
    "        )   \n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block_1(x) # [batch_size,512,38,38] (Conv4_3+ReLU輸出)\n",
    "        n=self.l2norm(x)\n",
    "        loc1=self.loc_1(n).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf1=self.conf_1(n).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        x=self.block_2(x) # [batch_size,1024,19,19] (Conv7+ReLU輸出)\n",
    "        loc2=self.loc_2(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf2=self.conf_2(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        x=self.block_3(x) # [batch_size,512,10,10] (Conv8_2+ReLU輸出)\n",
    "        loc3=self.loc_3(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf3=self.conf_3(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        x=self.block_4(x) # [batch_size,256,5,5] (Conv9_2+ReLU輸出)\n",
    "        loc4=self.loc_4(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf4=self.conf_4(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        x=self.block_5(x) # [batch_size,256,3,3] (Conv10_2+ReLU輸出)\n",
    "        loc5=self.loc_5(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf5=self.conf_5(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        x=self.block_6(x) # [batch_size,256,1,1] (Conv11_2+ReLU輸出)\n",
    "        loc6=self.loc_6(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf6=self.conf_6(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        loc=torch.cat((loc1,loc2,loc3,loc4,loc5,loc6),1) # [batch_size,8732,4]，import torch\n",
    "        conf=torch.cat((conf1,conf2,conf3,conf4,conf5,conf6),1) # [batch_size,8732,num_classes]，import torch\n",
    "        return loc,conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625dd0a4-2202-4148-8632-d00facfa0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Norm(nn.Module):\n",
    "    def __init__(self,in_channels,scale):\n",
    "        super(L2Norm,self).__init__()\n",
    "        self.in_channels=in_channels\n",
    "        self.gamma=scale or None\n",
    "        self.eps=1e-10\n",
    "        self.weight=nn.Parameter(torch.Tensor(self.in_channels)) # from torch import nn，import torch\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        nn.init.constant_(self.weight,self.gamma) # from torch import nn \n",
    "    def forward(self,x):\n",
    "        norm=x.pow(2).sum(dim=1,keepdim=True).sqrt()+self.eps\n",
    "        x=torch.div(x,norm) # import torch\n",
    "        out=self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x)*x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d7549-e579-4333-8bad-0a62f25f74b1",
   "metadata": {},
   "source": [
    "#### 使用模型進行訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01ceca1e-5f99-4278-9cea-ea45077917cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector=SSD().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff929376-6b1c-4b6e-bc17-2a942049fccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 載入模型\n",
    "detector.load_state_dict(torch.load(file_name)) # import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6a8f16-2675-41c1-91f4-15c4f4d55212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00bb264a-693c-4d03-85fd-3c08f4c00962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch:1\n",
      "Training Loss=0.02402198374822084\n",
      "Running Epoch:2\n",
      "Training Loss=0.03500946753713557\n",
      "Running Epoch:3\n",
      "Training Loss=0.02686885470389985\n",
      "Running Epoch:4\n",
      "Training Loss=0.020599678951296885\n",
      "Running Epoch:5\n",
      "Training Loss=0.007306065923560154\n",
      "Running Epoch:6\n",
      "Training Loss=0.10329399086898024\n",
      "Running Epoch:7\n",
      "Training Loss=0.023532801603450552\n",
      "Running Epoch:8\n",
      "Training Loss=0.006743446586399391\n",
      "Running Epoch:9\n",
      "Training Loss=0.006308640893332813\n",
      "Running Epoch:10\n",
      "Training Loss=0.02788489241700354\n",
      "Running Epoch:11\n",
      "Training Loss=0.030420383941357685\n",
      "Running Epoch:12\n",
      "Training Loss=0.037654699670884535\n",
      "Running Epoch:13\n",
      "Training Loss=0.05282324709418456\n",
      "Running Epoch:14\n",
      "Training Loss=0.025802557653091744\n",
      "Running Epoch:15\n",
      "Training Loss=0.012780187911446158\n",
      "Running Epoch:16\n",
      "Training Loss=0.009286643535452565\n",
      "Running Epoch:17\n",
      "Training Loss=0.04556660571765069\n",
      "Running Epoch:18\n",
      "Training Loss=0.02483699124906992\n",
      "Running Epoch:19\n",
      "Training Loss=0.012455425088749719\n",
      "Running Epoch:20\n",
      "Training Loss=0.012277153813205375\n",
      "Running Epoch:21\n",
      "Training Loss=0.010334630699358154\n",
      "Running Epoch:22\n",
      "Training Loss=0.008862133901648625\n",
      "Running Epoch:23\n",
      "Training Loss=0.1566999935146366\n",
      "Running Epoch:24\n",
      "Training Loss=0.023057711531829273\n",
      "Running Epoch:25\n",
      "Training Loss=0.012951227069705952\n",
      "Running Epoch:26\n",
      "Training Loss=0.01615935177522236\n",
      "Running Epoch:27\n",
      "Training Loss=0.06609090312910942\n",
      "Running Epoch:28\n",
      "Training Loss=0.03224401811657992\n",
      "Running Epoch:29\n",
      "Training Loss=0.00928475912303101\n",
      "Running Epoch:30\n",
      "Training Loss=0.00570500940406102\n",
      "Running Epoch:31\n",
      "Training Loss=0.0025031417398468678\n",
      "Running Epoch:32\n",
      "Training Loss=0.0036494629384694268\n",
      "Running Epoch:33\n",
      "Training Loss=0.004176869615406042\n",
      "Running Epoch:34\n",
      "Training Loss=0.0035465459896522676\n",
      "Running Epoch:35\n",
      "Training Loss=0.004109921192076551\n",
      "Running Epoch:36\n",
      "Training Loss=0.003759244918096704\n",
      "Running Epoch:37\n",
      "Training Loss=0.006000433349258482\n",
      "Running Epoch:38\n",
      "Training Loss=0.1050636378488856\n",
      "Running Epoch:39\n",
      "Training Loss=0.18795557145639652\n",
      "Running Epoch:40\n",
      "Training Loss=0.037072688385992925\n",
      "Running Epoch:41\n",
      "Training Loss=0.003807829659793661\n",
      "Running Epoch:42\n",
      "Training Loss=0.0017658210216832649\n",
      "Running Epoch:43\n",
      "Training Loss=0.00146292521047137\n",
      "Running Epoch:44\n",
      "Training Loss=0.001491700484502854\n",
      "Running Epoch:45\n",
      "Training Loss=0.0019087921193924181\n",
      "Running Epoch:46\n",
      "Training Loss=0.0021049263248548184\n",
      "Running Epoch:47\n",
      "Training Loss=0.00282140559868319\n",
      "Running Epoch:48\n",
      "Training Loss=0.003642246390259504\n",
      "Running Epoch:49\n",
      "Training Loss=0.004071175220547543\n",
      "Running Epoch:50\n",
      "Training Loss=0.004861004783090357\n",
      "Running Epoch:51\n",
      "Training Loss=0.10674256216036823\n",
      "Running Epoch:52\n",
      "Training Loss=0.06542603781978362\n",
      "Running Epoch:53\n",
      "Training Loss=0.007019206743739032\n",
      "Running Epoch:54\n",
      "Training Loss=0.001879351052306913\n",
      "Running Epoch:55\n",
      "Training Loss=0.001452455524267027\n",
      "Running Epoch:56\n",
      "Training Loss=0.0014106032064233071\n",
      "Running Epoch:57\n",
      "Training Loss=0.0019142992834747105\n",
      "Running Epoch:58\n",
      "Training Loss=0.002202030450533488\n",
      "Running Epoch:59\n",
      "Training Loss=0.0026050130265162113\n",
      "Running Epoch:60\n",
      "Training Loss=0.004131331180964007\n",
      "Running Epoch:61\n",
      "Training Loss=0.0032944028849186406\n",
      "Running Epoch:62\n",
      "Training Loss=0.0026656349286851943\n",
      "Running Epoch:63\n",
      "Training Loss=0.15129275484517307\n",
      "Running Epoch:64\n",
      "Training Loss=0.17112588686142902\n",
      "Running Epoch:65\n",
      "Training Loss=0.04901312575609883\n",
      "Running Epoch:66\n",
      "Training Loss=0.04265476306040066\n",
      "Running Epoch:67\n",
      "Training Loss=0.016180785943299487\n",
      "Running Epoch:68\n",
      "Training Loss=0.006603216946120134\n",
      "Running Epoch:69\n",
      "Training Loss=0.0018167518810271134\n",
      "Running Epoch:70\n",
      "Training Loss=0.0013916152997416967\n",
      "Running Epoch:71\n",
      "Training Loss=0.0013386409679747384\n",
      "Running Epoch:72\n",
      "Training Loss=0.0017021283160438352\n",
      "Running Epoch:73\n",
      "Training Loss=0.0023254215431897803\n",
      "Running Epoch:74\n",
      "Training Loss=0.002445633760003469\n",
      "Running Epoch:75\n",
      "Training Loss=0.004381754729002189\n",
      "Running Epoch:76\n",
      "Training Loss=0.004133604627014022\n",
      "Running Epoch:77\n",
      "Training Loss=0.0038223056004186343\n",
      "Running Epoch:78\n",
      "Training Loss=0.00643042769760181\n",
      "Running Epoch:79\n",
      "Training Loss=0.1050472957553764\n",
      "Running Epoch:80\n",
      "Training Loss=0.06281965401815606\n",
      "Running Epoch:81\n",
      "Training Loss=0.041680907651910695\n",
      "Running Epoch:82\n",
      "Training Loss=0.025368499753733627\n",
      "Running Epoch:83\n",
      "Training Loss=0.00723401626758322\n",
      "Running Epoch:84\n",
      "Training Loss=0.0031229887594736894\n",
      "Running Epoch:85\n",
      "Training Loss=0.001527740228037487\n",
      "Running Epoch:86\n",
      "Training Loss=0.0011482838381893833\n",
      "Running Epoch:87\n",
      "Training Loss=0.0013495517820502716\n",
      "Running Epoch:88\n",
      "Training Loss=0.0022640111239862508\n",
      "Running Epoch:89\n",
      "Training Loss=0.0025616756051731605\n",
      "Running Epoch:90\n",
      "Training Loss=0.0038002134878337506\n",
      "Running Epoch:91\n",
      "Training Loss=0.002388558503576794\n",
      "Running Epoch:92\n",
      "Training Loss=0.0024110141047933202\n",
      "Running Epoch:93\n",
      "Training Loss=0.004344780395250637\n",
      "Running Epoch:94\n",
      "Training Loss=0.0043005628350376015\n",
      "Running Epoch:95\n",
      "Training Loss=0.13238946451585945\n",
      "Running Epoch:96\n",
      "Training Loss=0.0731182181154846\n",
      "Running Epoch:97\n",
      "Training Loss=0.0077060812658093425\n",
      "Running Epoch:98\n",
      "Training Loss=0.0033407781777184914\n",
      "Running Epoch:99\n",
      "Training Loss=0.0013790558251782889\n",
      "Running Epoch:100\n",
      "Training Loss=0.0010904726982125604\n",
      "Running Epoch:101\n",
      "Training Loss=0.0010102462697893035\n",
      "Running Epoch:102\n",
      "Training Loss=0.0011393768337756007\n",
      "Running Epoch:103\n",
      "Training Loss=0.001593222664089862\n",
      "Running Epoch:104\n",
      "Training Loss=0.003121780626639193\n",
      "Running Epoch:105\n",
      "Training Loss=0.003435115374022174\n",
      "Running Epoch:106\n",
      "Training Loss=0.0032900981282295363\n",
      "Running Epoch:107\n",
      "Training Loss=0.0022902891758429136\n",
      "Running Epoch:108\n",
      "Training Loss=0.11971811553081373\n",
      "Running Epoch:109\n",
      "Training Loss=0.03396824965096323\n",
      "Running Epoch:110\n",
      "Training Loss=0.00761339478159099\n",
      "Running Epoch:111\n",
      "Training Loss=0.0021242170080335108\n",
      "Running Epoch:112\n",
      "Training Loss=0.001366337789301618\n",
      "Running Epoch:113\n",
      "Training Loss=0.0011700076104144443\n",
      "Running Epoch:114\n",
      "Training Loss=0.0012134089840025806\n",
      "Running Epoch:115\n",
      "Training Loss=0.0014322869670444906\n",
      "Running Epoch:116\n",
      "Training Loss=0.002985999779464111\n",
      "Running Epoch:117\n",
      "Training Loss=0.0025880740333106334\n",
      "Running Epoch:118\n",
      "Training Loss=0.0018437345206481983\n",
      "Running Epoch:119\n",
      "Training Loss=0.0023516185676313297\n",
      "Running Epoch:120\n",
      "Training Loss=0.0025186679335402317\n",
      "Running Epoch:121\n",
      "Training Loss=0.0035652248674102596\n",
      "Running Epoch:122\n",
      "Training Loss=0.1392892834826272\n",
      "Running Epoch:123\n",
      "Training Loss=0.059624234775911986\n",
      "Running Epoch:124\n",
      "Training Loss=0.012152923627651474\n",
      "Running Epoch:125\n",
      "Training Loss=0.0019635309478139816\n",
      "Running Epoch:126\n",
      "Training Loss=0.0011020073061497316\n",
      "Running Epoch:127\n",
      "Training Loss=0.0008754646699676596\n",
      "Running Epoch:128\n",
      "Training Loss=0.0009074321484779066\n",
      "Running Epoch:129\n",
      "Training Loss=0.0011392606683543983\n",
      "Running Epoch:130\n",
      "Training Loss=0.0013400629929838485\n",
      "Running Epoch:131\n",
      "Training Loss=0.0020825543183952224\n",
      "Running Epoch:132\n",
      "Training Loss=0.002801846951195288\n",
      "Running Epoch:133\n",
      "Training Loss=0.0025136493854315944\n",
      "Running Epoch:134\n",
      "Training Loss=0.0022711954340278607\n",
      "Running Epoch:135\n",
      "Training Loss=0.0025393596386677634\n",
      "Running Epoch:136\n",
      "Training Loss=0.01677107891674647\n",
      "Running Epoch:137\n",
      "Training Loss=0.14015776681002357\n",
      "Running Epoch:138\n",
      "Training Loss=0.030521430238216837\n",
      "Running Epoch:139\n",
      "Training Loss=0.020762249695388226\n",
      "Running Epoch:140\n",
      "Training Loss=0.012532801263225327\n",
      "Running Epoch:141\n",
      "Training Loss=0.0016268465034022614\n",
      "Running Epoch:142\n",
      "Training Loss=0.0009940421805228685\n",
      "Running Epoch:143\n",
      "Training Loss=0.0008736931108901888\n",
      "Running Epoch:144\n",
      "Training Loss=0.0010092950311797024\n",
      "Running Epoch:145\n",
      "Training Loss=0.001305732864598934\n",
      "Running Epoch:146\n",
      "Training Loss=0.0017779607212103736\n",
      "Running Epoch:147\n",
      "Training Loss=0.0017682386149988861\n",
      "Running Epoch:148\n",
      "Training Loss=0.002413402260085265\n",
      "Running Epoch:149\n",
      "Training Loss=0.00334041934158178\n",
      "Running Epoch:150\n",
      "Training Loss=0.02152225930847176\n",
      "Running Epoch:151\n",
      "Training Loss=0.09513701151733253\n",
      "Running Epoch:152\n",
      "Training Loss=0.016432995585204738\n",
      "Running Epoch:153\n",
      "Training Loss=0.005492781096491536\n",
      "Running Epoch:154\n",
      "Training Loss=0.002652720330212039\n",
      "Running Epoch:155\n",
      "Training Loss=0.006330163556976461\n",
      "Running Epoch:156\n",
      "Training Loss=0.027369123345263657\n",
      "Running Epoch:157\n",
      "Training Loss=0.04675432430554883\n",
      "Running Epoch:158\n",
      "Training Loss=0.048661155410486684\n",
      "Running Epoch:159\n",
      "Training Loss=0.034551536371770275\n",
      "Running Epoch:160\n",
      "Training Loss=0.019906125103419978\n",
      "Running Epoch:161\n",
      "Training Loss=0.006170887992976422\n",
      "Running Epoch:162\n",
      "Training Loss=0.0014365168982445856\n",
      "Running Epoch:163\n",
      "Training Loss=0.0009999004473171178\n",
      "Running Epoch:164\n",
      "Training Loss=0.001190944988125611\n",
      "Running Epoch:165\n",
      "Training Loss=0.0016687459068753033\n",
      "Running Epoch:166\n",
      "Training Loss=0.0016516485350762083\n",
      "Running Epoch:167\n",
      "Training Loss=0.001968171485049969\n",
      "Running Epoch:168\n",
      "Training Loss=0.0017971155937014497\n",
      "Running Epoch:169\n",
      "Training Loss=0.002716108921691143\n",
      "Running Epoch:170\n",
      "Training Loss=0.0033320320098641745\n",
      "Running Epoch:171\n",
      "Training Loss=0.0028734766503900155\n",
      "Running Epoch:172\n",
      "Training Loss=0.002692410308855673\n",
      "Running Epoch:173\n",
      "Training Loss=0.05582912184627362\n",
      "Running Epoch:174\n",
      "Training Loss=0.07776584847069863\n",
      "Running Epoch:175\n",
      "Training Loss=0.03636938373689107\n",
      "Running Epoch:176\n",
      "Training Loss=0.028877018969020778\n",
      "Running Epoch:177\n",
      "Training Loss=0.0013491083940969206\n",
      "Running Epoch:178\n",
      "Training Loss=0.0008909268728797956\n",
      "Running Epoch:179\n",
      "Training Loss=0.0007344827609182109\n",
      "Running Epoch:180\n",
      "Training Loss=0.0007428901526179025\n",
      "Running Epoch:181\n",
      "Training Loss=0.0011639058057719175\n",
      "Running Epoch:182\n",
      "Training Loss=0.0014376012594580435\n",
      "Running Epoch:183\n",
      "Training Loss=0.002213903408501801\n",
      "Running Epoch:184\n",
      "Training Loss=0.0020305052664018296\n",
      "Running Epoch:185\n",
      "Training Loss=0.001996595803182461\n",
      "Running Epoch:186\n",
      "Training Loss=0.002978925788743924\n",
      "Running Epoch:187\n",
      "Training Loss=0.0024678812226027707\n",
      "Running Epoch:188\n",
      "Training Loss=0.0024785469984802654\n",
      "Running Epoch:189\n",
      "Training Loss=0.0032895410433244655\n",
      "Running Epoch:190\n",
      "Training Loss=0.08455818461670596\n",
      "Running Epoch:191\n",
      "Training Loss=0.0880451787832544\n",
      "Running Epoch:192\n",
      "Training Loss=0.0172437947520801\n",
      "Running Epoch:193\n",
      "Training Loss=0.03495937371185765\n",
      "Running Epoch:194\n",
      "Training Loss=0.00503092677340029\n",
      "Running Epoch:195\n",
      "Training Loss=0.0017787929070678524\n",
      "Running Epoch:196\n",
      "Training Loss=0.0008397156282349883\n",
      "Running Epoch:197\n",
      "Training Loss=0.0007374416395986674\n",
      "Running Epoch:198\n",
      "Training Loss=0.0010774244070649646\n",
      "Running Epoch:199\n",
      "Training Loss=0.0013669050419609639\n",
      "Running Epoch:200\n",
      "Training Loss=0.001118479119688335\n"
     ]
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(detector.parameters(),lr=learning_rate) # import torch\n",
    "#optimizer=torch.optim.SGD(detector.parameters(),lr=learning_rate) # import torch\n",
    "scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size,0.1) # import torch\n",
    "num_anchor=anchor.size(0) # 8732，anchor為[8732,4]\n",
    "for i in range(1,epochs+1):\n",
    "    print('Running Epoch:'+str(i))\n",
    "    train_loss,train_batch,valid_loss,valid_batch=0,0,0,0\n",
    "    detector.train()\n",
    "    for img,bbox_,cls_ in train_loader: # 一個batch的img、bbox_、cls_，img：[batch_size,3,300,300]，bbox_：列表長度為batch_size，列表中每個元素為一個[該影像中的物件個數,4]的tensor，cls_：列表長度為batch_size，列表中每個元素為一個[該影像中的物件個數]的tensor\n",
    "        if img.size(0)!=batch_size: # 最後不足一個batch的訓練影像不進行訓練\n",
    "            break\n",
    "        img=img.to(device)\n",
    "        gHat=torch.Tensor(batch_size,num_anchor,4) # import torch\n",
    "        matched_class=torch.LongTensor(batch_size,num_anchor) # import torch\n",
    "        pos=list()\n",
    "        for j in range(batch_size):\n",
    "\n",
    "            # 找出每個錨框匹配的物件(以比較IoU為主，但若IoU值為該物件的最大IoU值，則直接指定對應該物件。若IoU為0，則隨便匹配物件，待之後用threshold去除)\n",
    "            bbox=bbox_[j].to(device) # [該影像中的物件個數,4]\n",
    "            cls=cls_[j].to(device) # [該影像中的物件個數]\n",
    "            num_objects=bbox.size(0) # 該影像中的物件個數，[1]。bbox：[該影像中的物件個數,4]\n",
    "            min_xy=torch.max(bbox[:,:2].unsqueeze(1).expand(num_objects,num_anchor,2),anchor[:,:2].broadcast_to(num_objects,num_anchor,2)) # 每個物件與8732個錨框比較取較大的xmin與ymin，[該影像中的物件個數,8732,2]，2表示較大的xmin與ymin，import torch\n",
    "            max_xy=torch.min(bbox[:,2:].unsqueeze(1).expand(num_objects,num_anchor,2),anchor[:,2:].broadcast_to(num_objects,num_anchor,2)) # 每個物件與8732個錨框比較取較小的xmax與ymax，[該影像中的物件個數,8732,2]，2表示較小xmax與ymax，import torch\n",
    "            side_length=(max_xy-min_xy).clamp(min=0) # 交集面積的邊長，[該影像中的物件個數,8732,2]\n",
    "            area_inter=side_length[:,:,0]*side_length[:,:,1] # 交集面積，[該影像中的物件個數,8732]\n",
    "            area_bbox=((bbox[:,2]-bbox[:,0])*(bbox[:,3]-bbox[:,1])).unsqueeze(1).expand(num_objects,num_anchor)\n",
    "            area_anchor=((anchor[:,2]-anchor[:,0])*(anchor[:,3]-anchor[:,1])).broadcast_to(num_objects,num_anchor)\n",
    "            IoU=area_inter/(area_bbox+area_anchor-area_inter) # IOU，[該影像中的物件個數,8732]\n",
    "            maxIoU_object,anchor_idx=torch.max(IoU,dim=1) # dim=1表示取每列的最大值。maxIoU_object為每個物件的最大IoU，[該影像中的物件個數]。anchor_idx為每個物件最大IoU的錨框編號，[該影像中的物件個數]。import torch\n",
    "            maxIoU_anchor,object_idx=torch.max(IoU,dim=0) # dim=0表示取每行的最大值。maxIoU_anchor為每個錨框的最大IoU值，[8732]。object_idx為每個錨框最大IoU的物件編號(非類別)(0,1,2,...)，[8732]。import torch\n",
    "            maxIoU_anchor.index_fill_(0,anchor_idx,2) # 修改maxIoU_anchor(每個錨框的最大IoU)，令每個物件最大IoU的錨框(即anchor_idx)的IoU為2\n",
    "            pos.append(maxIoU_anchor>=threshold) # True/False，利用threshold篩選出背景，令有匹配到物件的錨框為True，背景為False(大部分為False)，pos：列表長度為batch_size，列表中每個元素為[8732]的tensor\n",
    "            for k in range(num_objects): # k:0~(num_objects-1)\n",
    "                object_idx[anchor_idx[k]]=k # 將每個物件IoU=2的錨框所對應的物件指定為該物件\n",
    "            matched_bbox=bbox[object_idx] # 每個錨框匹配物件的邊界框，[8732,4]，[xmin ymin xmax ymax]\n",
    "            matched_class[j]=cls[object_idx] # 每個錨框匹配物件的類別(1,2, ...)，matched_class[0]：[8732]，matched_class：[batch_size,8732]\n",
    "            matched_class[j][maxIoU_anchor<threshold]=0 # 利用threshold決定那些錨框匹配的物件類別為背景(0)，matched_class表示每個錨框匹配物件的類別(0,1,2, ...)，[batch_size,8732]\n",
    "            gHat_cx=((matched_bbox[:,0]+matched_bbox[:,2])/2-(anchor[:,0]+anchor[:,2])/2)/((anchor[:,2]-anchor[:,0])*variances[0]) # [8732]\n",
    "            gHat_cy=((matched_bbox[:,1]+matched_bbox[:,3])/2-(anchor[:,1]+anchor[:,3])/2)/((anchor[:,3]-anchor[:,1])*variances[0]) # [8732]\n",
    "            gHat_w=torch.log((matched_bbox[:,2]-matched_bbox[:,0])/(anchor[:,2]-anchor[:,0]))/variances[1] # [8732]，import torch\n",
    "            gHat_h=torch.log((matched_bbox[:,3]-matched_bbox[:,1])/(anchor[:,3]-anchor[:,1]))/variances[1] # [8732]，import torch\n",
    "            gHat[j]=torch.stack((gHat_cx,gHat_cy,gHat_w,gHat_h),1) # gHat[0]：[8732,4]，gHat：[batch_size,8732,4]，import torch\n",
    "        pos=torch.stack(pos,0) # 將pos從list轉為tensor，pos：[batch_size,8732]，True/False，import torch\n",
    "        pred_loc,pred_conf=detector(img) # pred_loc：[batch_size,8732,4]，pred_conf：[batch_size,8732,num_classes]\n",
    "        gHat,matched_class=gHat.to(device),matched_class.to(device) # gHat：[batch_size,8732,4]，matched_class：[batch_size,8732]\n",
    "        num_pos=pos.sum(dim=1,keepdim=True) # 利用threshold篩選後有匹配到物件的錨框數量，[batch_size,1]\n",
    "        pos_expand=pos.unsqueeze(pos.dim()).expand_as(pred_loc) # 將[batch_size,8732]的pos擴增成[batch_size,8732,4]，Ture/False\n",
    "        pos_l=pred_loc[pos_expand].view(-1,4) # 取出有匹配到物件的錨框的pred_loc，[batch內有匹配到物件的錨框總數(即num_pos內值的加總),4]\n",
    "        pos_gHat=gHat[pos_expand].view(-1,4) # 取出有匹配到物件的錨框的gHat，[batch內有匹配到物件的錨框總數(即num_pos內值的加總),4]\n",
    "        L_loc=torch.nn.functional.smooth_l1_loss(pos_l,pos_gHat) # 計算一個batch內有匹配到物件的錨框的L_loc，[1]，import torch\n",
    "        batch_pred_conf=pred_conf.view(-1,num_classes) # 將pred_conf內的batch整合在一起，[batch_size*8732,num_classes]\n",
    "        crossEntropy=torch.logsumexp(batch_pred_conf,dim=1,keepdim=True)-batch_pred_conf.gather(1,matched_class.view(-1,1)) # 計算每個錨框匹配物件(包含背景)的負logsumexp，[batch_size*8732,1]，import torch\n",
    "            # matched_class.view(-1,1)：將matched_class(每個錨框匹配物件的類別(0,1,2, ...))內的batch整合在一起，[batch_size*8732,1]\n",
    "            # batch_pred_conf.gather(1,matched_class.view(-1,1))：根據matched_class.view(-1,1)(每個錨框匹配物件的類別(0,1,2, ...))取出該物件類別的預測置信值(pred_conf)\n",
    "        crossEntropy_pos=crossEntropy[pos.view(-1,1)].sum(dim=0,keepdim=True)\n",
    "        crossEntropy_neg=crossEntropy\n",
    "        crossEntropy_neg[pos.view(-1,1)]=0 # 利用threshold篩選後若錨框有匹配到物件，則令該錨框匹配物件的負logsumexp為0，[batch_size*8732,1]\n",
    "        crossEntropy_neg=crossEntropy_neg.view(batch_size,-1) # 將crossEntropy從[batch_size*8732,1]轉換成[batch_size,8732]        \n",
    "        _,background_idx=crossEntropy_neg.sort(1,descending=True) # background_idx：[batch_size,8732]，將有匹配到物件的錨框的負logsumexp設為0後，依負logsumexp由大而小排列並取得錨框編號(如編號5即表示第5個錨框所匹配的背景的負logsumexp為最大)\n",
    "        _,idx_rank=background_idx.sort(1) # idx_rank：[batch_size,8732]，依crossEntropy由小而大排序，如4、1、3、2表示第1個錨框在負logsumexp中排第4(愈大表示負logsumexp愈小)，第2個錨框在負logsumexp中排第1\n",
    "        num_neg=torch.clamp(3*num_pos,max=pos.size(1)-num_pos) # num_neg：[batch_size,1]，定義每張影像的負樣本個數為正樣本個數的3倍，上限改為錨框個數-正樣本個數，import torch\n",
    "        neg=idx_rank<num_neg.expand_as(idx_rank) # neg：True/False，將負logsumexp最大的前num_neg個設為True，[batch_size,8732]\n",
    "        crossEntropy_neg=crossEntropy[neg.view(-1,1)].sum(dim=0,keepdim=True)\n",
    "        L_conf=crossEntropy_pos+crossEntropy_neg\n",
    "        N=num_pos.data.sum()\n",
    "        loss=(L_conf+alpha*L_loc)/N\n",
    "        train_loss+=loss.item()\n",
    "        train_batch+=1\n",
    "        optimizer.zero_grad() # 權重梯度歸零\n",
    "        loss.backward() # 計算每個權重的loss梯度\n",
    "        optimizer.step() # 權重更新\n",
    "    scheduler.step()\n",
    "    if train_batch!=0:\n",
    "        print('Training Loss='+str(train_loss/train_batch)) # 計算每一個epoch的平均訓練loss\n",
    "\n",
    "    detector.eval()\n",
    "    for img,bbox_,cls_ in valid_loader: # 一個batch的img、bbox_、cls_，img：[batch_size,3,300,300]，bbox_：列表長度為batch_size，列表中每個元素為一個[該影像中的物件個數,4]的tensor，cls_：列表長度為batch_size，列表中每個元素為一個[該影像中的物件個數]的tensor\n",
    "        if img.size(0)!=batch_size: # 最後不足一個batch的驗證影像不進行驗證\n",
    "            break \n",
    "        img=img.to(device)\n",
    "        gHat=torch.Tensor(batch_size,num_anchor,4) # import torch\n",
    "        matched_class=torch.LongTensor(batch_size,num_anchor) # import torch\n",
    "        pos=list()\n",
    "        for j in range(batch_size):\n",
    "\n",
    "            # 找出每個錨框匹配的物件(以比較IoU為主，但若IoU值為該物件的最大IoU值，則直接指定對應該物件。若IoU為0，則隨便匹配物件，待之後用threshold去除)\n",
    "            bbox=bbox_[j].to(device) # [該影像中的物件個數,4]\n",
    "            cls=cls_[j].to(device) # [該影像中的物件個數]\n",
    "            num_objects=bbox.size(0) # 該影像中的物件個數，[1]。bbox：[該影像中的物件個數,4]\n",
    "            min_xy=torch.max(bbox[:,:2].unsqueeze(1).expand(num_objects,num_anchor,2),anchor[:,:2].broadcast_to(num_objects,num_anchor,2)) # 每個物件與8732個錨框比較取較大的xmin與ymin，[該影像中的物件個數,8732,2]，2表示較大的xmin與ymin，import torch\n",
    "            max_xy=torch.min(bbox[:,2:].unsqueeze(1).expand(num_objects,num_anchor,2),anchor[:,2:].broadcast_to(num_objects,num_anchor,2)) # 每個物件與8732個錨框比較取較小的xmax與ymax，[該影像中的物件個數,8732,2]，2表示較小xmax與ymax，import torch\n",
    "            side_length=(max_xy-min_xy).clamp(min=0) # 交集面積的邊長，[該影像中的物件個數,8732,2]\n",
    "            area_inter=side_length[:,:,0]*side_length[:,:,1] # 交集面積，[該影像中的物件個數,8732]\n",
    "            area_bbox=((bbox[:,2]-bbox[:,0])*(bbox[:,3]-bbox[:,1])).unsqueeze(1).expand(num_objects,num_anchor)\n",
    "            area_anchor=((anchor[:,2]-anchor[:,0])*(anchor[:,3]-anchor[:,1])).broadcast_to(num_objects,num_anchor)\n",
    "            IoU=area_inter/(area_bbox+area_anchor-area_inter) # IOU，[該影像中的物件個數,8732]\n",
    "            maxIoU_object,anchor_idx=torch.max(IoU,dim=1) # dim=1表示取每列的最大值。maxIoU_object為每個物件的最大IoU，[該影像中的物件個數]。anchor_idx為每個物件最大IoU的錨框編號，[該影像中的物件個數]。import torch\n",
    "            maxIoU_anchor,object_idx=torch.max(IoU,dim=0) # dim=0表示取每行的最大值。maxIoU_anchor為每個錨框的最大IoU值，[8732]。object_idx為每個錨框最大IoU的物件編號(非類別)(0,1,2,...)，[8732]。import torch\n",
    "            maxIoU_anchor.index_fill_(0,anchor_idx,2) # 修改maxIoU_anchor(每個錨框的最大IoU)，令每個物件最大IoU的錨框(即anchor_idx)的IoU為2\n",
    "            pos.append(maxIoU_anchor>=threshold) # True/False，利用threshold篩選出背景，令有匹配到物件的錨框為True，背景為False(大部分為False)，pos：列表長度為batch_size，列表中每個元素為[8732]的tensor\n",
    "            for k in range(num_objects): # k:0~(num_objects-1)\n",
    "                object_idx[anchor_idx[k]]=k # 將每個物件IoU=2的錨框所對應的物件指定為該物件\n",
    "            matched_bbox=bbox[object_idx] # 每個錨框匹配物件的邊界框，[8732,4]，[xmin ymin xmax ymax]\n",
    "            matched_class[j]=cls[object_idx] # 每個錨框匹配物件的類別(1,2, ...)，matched_class[0]：[8732]，matched_class：[batch_size,8732]\n",
    "            matched_class[j][maxIoU_anchor<threshold]=0 # 利用threshold決定那些錨框匹配的物件類別為背景(0)，matched_class表示每個錨框匹配物件的類別(0,1,2, ...)，[batch_size,8732]\n",
    "            gHat_cx=((matched_bbox[:,0]+matched_bbox[:,2])/2-(anchor[:,0]+anchor[:,2])/2)/((anchor[:,2]-anchor[:,0])*variances[0]) # [8732]\n",
    "            gHat_cy=((matched_bbox[:,1]+matched_bbox[:,3])/2-(anchor[:,1]+anchor[:,3])/2)/((anchor[:,3]-anchor[:,1])*variances[0]) # [8732]\n",
    "            gHat_w=torch.log((matched_bbox[:,2]-matched_bbox[:,0])/(anchor[:,2]-anchor[:,0]))/variances[1] # [8732]，import torch\n",
    "            gHat_h=torch.log((matched_bbox[:,3]-matched_bbox[:,1])/(anchor[:,3]-anchor[:,1]))/variances[1] # [8732]，import torch\n",
    "            gHat[j]=torch.stack((gHat_cx,gHat_cy,gHat_w,gHat_h),1) # gHat[0]：[8732,4]，gHat：[batch_size,8732,4]，import torch\n",
    "        pos=torch.stack(pos,0) # 將pos從list轉為tensor，pos：[batch_size,8732]，True/False，import torch\n",
    "        pred_loc,pred_conf=detector(img) # pred_loc：[batch_size,8732,4]，pred_conf：[batch_size,8732,num_classes]\n",
    "        gHat,matched_class=gHat.to(device),matched_class.to(device) # gHat：[batch_size,8732,4]，matched_class：[batch_size,8732]\n",
    "        num_pos=pos.sum(dim=1,keepdim=True) # 利用threshold篩選後有匹配到物件的錨框數量，[batch_size,1]\n",
    "        pos_expand=pos.unsqueeze(pos.dim()).expand_as(pred_loc) # 將[batch_size,8732]的pos擴增成[batch_size,8732,4]，Ture/False\n",
    "        pos_l=pred_loc[pos_expand].view(-1,4) # 取出有匹配到物件的錨框的pred_loc，[batch內有匹配到物件的錨框總數(即num_pos內值的加總),4]\n",
    "        pos_gHat=gHat[pos_expand].view(-1,4) # 取出有匹配到物件的錨框的gHat，[batch內有匹配到物件的錨框總數(即num_pos內值的加總),4]\n",
    "        L_loc=torch.nn.functional.smooth_l1_loss(pos_l,pos_gHat) # 計算一個batch內有匹配到物件的錨框的L_loc，[1]，import torch\n",
    "        batch_pred_conf=pred_conf.view(-1,num_classes) # 將pred_conf內的batch整合在一起，[batch_size*8732,num_classes]\n",
    "        crossEntropy=torch.logsumexp(batch_pred_conf,dim=1,keepdim=True)-batch_pred_conf.gather(1,matched_class.view(-1,1)) # 計算每個錨框匹配物件(包含背景)的負logsumexp，[batch_size*8732,1]，import torch\n",
    "            # matched_class.view(-1,1)：將matched_class(每個錨框匹配物件的類別(0,1,2, ...))內的batch整合在一起，[batch_size*8732,1]\n",
    "            # batch_pred_conf.gather(1,matched_class.view(-1,1))：根據matched_class.view(-1,1)(每個錨框匹配物件的類別(0,1,2, ...))取出該物件類別的預測置信值(pred_conf)\n",
    "        crossEntropy_pos=crossEntropy[pos.view(-1,1)].sum(dim=0,keepdim=True)\n",
    "        crossEntropy_neg=crossEntropy\n",
    "        crossEntropy_neg[pos.view(-1,1)]=0 # 利用threshold篩選後若錨框有匹配到物件，則令該錨框匹配物件的負logsumexp為0，[batch_size*8732,1]\n",
    "        crossEntropy_neg=crossEntropy_neg.view(batch_size,-1) # 將crossEntropy從[batch_size*8732,1]轉換成[batch_size,8732]        \n",
    "        _,background_idx=crossEntropy_neg.sort(1,descending=True) # background_idx：[batch_size,8732]，將有匹配到物件的錨框的負logsumexp設為0後，依負logsumexp由大而小排列並取得錨框編號(如編號5即表示第5個錨框所匹配的背景的負logsumexp為最大)\n",
    "        _,idx_rank=background_idx.sort(1) # idx_rank：[batch_size,8732]，依crossEntropy由小而大排序，如4、1、3、2表示第1個錨框在負logsumexp中排第4(愈大表示負logsumexp愈小)，第2個錨框在負logsumexp中排第1\n",
    "        num_neg=torch.clamp(3*num_pos,max=pos.size(1)-num_pos) # num_neg：[batch_size,1]，定義每張影像的負樣本個數為正樣本個數的3倍，上限改為錨框個數-正樣本個數，import torch\n",
    "        neg=idx_rank<num_neg.expand_as(idx_rank) # neg：True/False，將負logsumexp最大的前num_neg個設為True，[batch_size,8732]\n",
    "        crossEntropy_neg=crossEntropy[neg.view(-1,1)].sum(dim=0,keepdim=True)\n",
    "        L_conf=crossEntropy_pos+crossEntropy_neg\n",
    "        N=num_pos.data.sum()\n",
    "        loss=(L_conf+alpha*L_loc)/N\n",
    "        valid_loss+=loss.item()\n",
    "        valid_batch+=1\n",
    "    if valid_batch!=0:\n",
    "        print('Validation Loss='+str(valid_loss/valid_batch)) # 計算每一個epoch的平均驗證loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f15e2808-cbae-474f-90fb-a9ebb329ee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "運行時間: 6770 秒\n"
     ]
    }
   ],
   "source": [
    "finish_time = time.time()\n",
    "print('運行時間:', int(finish_time - start_time), '秒')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b67e3-d765-4e23-ba2b-398bd8a9dced",
   "metadata": {},
   "source": [
    "#### 儲存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96cf40ee-657e-43fd-b050-ed7e9e27148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(detector.state_dict(),file_name) # import torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-final-report",
   "language": "python",
   "name": "ml-final-report"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
