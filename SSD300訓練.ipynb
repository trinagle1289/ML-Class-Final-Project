{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb5af25e-c4d6-4f8f-b668-ba81f28eddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee5264-8074-49da-b820-5a9b7f5b7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='PeopleCounting_202305031650.pth' # 副檔名通常以.pt或.pth儲存，建議使用.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a4d61-c593-42c8-9506-b98fc11a6634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "device=torch.device('cuda') # 'cuda'/'cpu'，import torch\n",
    "num_classes=2 # 物件類別數+1(背景)\n",
    "train_size=500\n",
    "valid_size=0\n",
    "batch_size=10\n",
    "learning_rate=0.001\n",
    "step_size=100 # Reriod of learning rate decay\n",
    "threshold=0.5 # 錨框匹配為物件/背景的閥值，參考值=0.5\n",
    "variances=[0.1,0.2] # 設定gHat中cx、cy與w、h間的權重\n",
    "epochs=1000\n",
    "TrainingImage='D:\\TrainingImage/'\n",
    "Annotation='D:\\Annotation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d90b1-cdf3-4f03-b623-6bc9fbf361ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transforms=transforms.Compose([transforms.Resize((300,300)),transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]) # ToTensor將影像像素歸一化至0~1(直接除以255)，from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5302e-8d35-4e45-aa73-e6b0b972eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立dataset\n",
    "from torch.utils.data import Dataset\n",
    "class ImageLabel(Dataset): # from torch.utils.data import Dataset\n",
    "    def __init__(self,img,bbox,cls):\n",
    "        self.img=img\n",
    "        self.bbox=bbox\n",
    "        self.cls=cls\n",
    "    def __getitem__(self,idx):\n",
    "        return self.img[idx],self.bbox[idx],self.cls[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "def collate_fn(batch):\n",
    "    img=list()\n",
    "    bbox=list()\n",
    "    cls=list()\n",
    "    for data in batch:\n",
    "        img.append(data[0])\n",
    "        bbox.append(data[1])\n",
    "        cls.append(data[2])\n",
    "    img=torch.stack(img,dim=0) # import torch\n",
    "    return img,bbox,cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f24505-a4c7-49b1-955c-abe311beefcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "all_image_name=os.listdir(TrainingImage) # 所有影像檔名(含.jpg)，import os\n",
    "img=list()\n",
    "bbox=list()\n",
    "cls=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e4b16-51e3-4c91-843d-8fa080c12ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "for image_name in all_image_name:\n",
    "    chi_en=list()\n",
    "    chi_hua=list()\n",
    "    I=Image.open(TrainingImage+image_name,mode='r') # from PIL import Image\n",
    "    I=transforms(I)\n",
    "    img.append(I) # 列表長度為影像個數，列表中每個元素為一個[3,300,300]的tensor\n",
    "    image_name=image_name[:-4] # 移除4個字元(.jpg)\n",
    "    root=ET.parse(Annotation+image_name+'.xml').getroot() # 獲取xml文件物件的根結點，import xml.etree.ElementTree as ET\n",
    "    size=root.find('size') # 獲取size子結點\n",
    "    width=int(size.find('width').text) # 原始影像的寬(像素)\n",
    "    height=int(size.find('height').text) # 原始影像的高(像素)\n",
    "    width_scale=I.size(1)/width # 輸入影像與原始影像的寬比\n",
    "    height_scale=I.size(1)/height # 輸入影像與原始影像的高比\n",
    "    for object in root.iter('object'): # 遞迴查詢所有的object子結點\n",
    "        bndbox=object.find('bndbox')\n",
    "        chi_en.append([int(bndbox.find('xmin').text)*width_scale,int(bndbox.find('ymin').text)*height_scale,int(bndbox.find('xmax').text)*width_scale,int(bndbox.find('ymax').text)*height_scale])    \n",
    "        name=object.find('name').text # 1,2,3,4,...  \n",
    "        chi_hua.append(int(name))\n",
    "    chi_en=torch.Tensor(chi_en) # 將chi_en轉成tensor，[該影像中的物件個數,4]，import torch\n",
    "    chi_hua=torch.Tensor(chi_hua) # 將chi_hua轉成tensor，import torch\n",
    "    bbox.append(chi_en) # 列表長度為影像個數，列表中每個元素為一個[該影像中的物件個數,4]的tensor\n",
    "    cls.append(chi_hua) # 列表長度為影像個數，列表中每個元素為一個[該影像中的物件個數]的tensor\n",
    "dataset=ImageLabel(img,bbox,cls)\n",
    "train_data,valid_data=torch.utils.data.random_split(dataset,[train_size,valid_size]) # import torch\n",
    "train_loader=torch.utils.data.DataLoader(train_data,batch_size,shuffle=True,collate_fn=collate_fn) # imort torch\n",
    "valid_loader=torch.utils.data.DataLoader(valid_data,batch_size,shuffle=False,collate_fn=collate_fn) # imort torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26d333-8c16-4f0e-b2ee-37beab1dc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立錨框\n",
    "feature_scale=[38,19,10,5,3,1] # 預測用的特徵圖尺寸(以像素為單位)\n",
    "sk=[0.07,0.15,0.33,0.51,0.69,0.87,1.05] # 各預測特徵圖的默認框尺度(相對於輸入影像的比例)，比預測特徵圖的個數多1\n",
    "aspect_ratio=[[1,2,1/2],[1,2,3,1/2,1/3],[1,2,3,1/2,1/3],[1,2,3,1/2,1/3],[1,2,1/2],[1,2,1/2]] # 各預測特徵圖的縱橫比(須檢查loc、conf的濾波器個數)\n",
    "abox=[]\n",
    "import itertools\n",
    "import math\n",
    "for i,j in enumerate(feature_scale):\n",
    "    for m,n in itertools.product(range(j),repeat=2):\n",
    "        cx=(n+0.5)/j # 等同於cx相對於輸入影像的比例位置(乘以輸入影像尺寸即為cx在輸入影像的像素位置)\n",
    "        cy=(m+0.5)/j # 等同於cy相對於輸入影像的比例位置(乘以輸入影像尺寸即為cy在輸入影像的像素位置)\n",
    "        for ar in aspect_ratio[i]:\n",
    "            abox+=[cx-sk[i]*math.sqrt(ar)/2,cy-sk[i]/math.sqrt(ar)/2,cx+sk[i]*math.sqrt(ar)/2,cy+sk[i]/math.sqrt(ar)/2] # [cxmin cymin cxmax cymax]\n",
    "        abox+=[cx-math.sqrt(sk[i]*sk[i+1])/2,cy-math.sqrt(sk[i]*sk[i+1])/2,cx+math.sqrt(sk[i]*sk[i+1])/2,cy+math.sqrt(sk[i]*sk[i+1])/2] # [xmin ymin xmax ymax]\n",
    "anchor=torch.Tensor(abox).view(-1,4).to(device) # [8732,4] (所有錨框的[xmin ymin xmax ymax]，皆相對於輸入影像的比例位置，乘以輸入影像尺寸即為在輸入影像的像素位置)，import torch\n",
    "anchor.clamp_(max=1, min=0) # 限定最大值為1、最小值0\n",
    "anchor=anchor*(img[0].size(1)) # 轉換成輸入影像尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf90044-04c8-451a-af27-0cc771fc8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class SSD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # block_1：Conv1_1~Conv4_3+ReLU\n",
    "        self.block_1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3,stride=1,padding=1), # [batch_size,64,300,300]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding=1), # [batch_size,64,300,300]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # [batch_size,64,150,150]\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,stride=1,padding=1), # [batch_size,128,150,150]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,stride=1,padding=1), # [batch_size,128,150,150]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # [batch_size,128,75,75]\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=1,padding=1), # [batch_size,256,75,75]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,stride=1,padding=1), # [batch_size,256,75,75]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,stride=1,padding=1), # [batch_size,256,75,75]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2,ceil_mode=True), # [batch_size,256,38,38]\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,38,38]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,38,38] \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,38,38]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Layer learns to scale the L2 normalized features from conv4_3\n",
    "        self.l2norm=L2Norm(512,20) # 512為輸入的特徵圖個數，20為scale\n",
    "         \n",
    "        # block_2：Pool4~Conv7+ReLU\n",
    "        self.block_2=nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2), # [batch_size,512,19,19]\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,stride=1,padding=1), # [batch_size,512,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=1,padding=1), # [batch_size,512,19,19]\n",
    "            nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3,stride=1,padding=6,dilation=6), # [batch_size,1024,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=1024,out_channels=1024,kernel_size=1,stride=1), # [batch_size,1024,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # block_3：Conv8_1~Conv8_2+ReLU\n",
    "        self.block_3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024,out_channels=256,kernel_size=1), # [batch_size,256,19,19]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,stride=2,padding=1), # [batch_size,512,10,10]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # block_4：Conv9_1~Conv9_2+ReLU\n",
    "        self.block_4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=128,kernel_size=1), # [batch_size,128,10,10]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,stride=2,padding=1), # [batch_size,256,5,5]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # block_5：Conv10_1~Conv10_2+ReLU\n",
    "        self.block_5=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=128,kernel_size=1), # [batch_size,128,5,5]\n",
    "            nn.ReLU(inplace=True),                            \n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3), # [batch_size,256,3,3]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # block_6：Conv11_1~Conv11_2+ReLU\n",
    "        self.block_6=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=128,kernel_size=1), # [batch_size,128,3,3]                            \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3), # [batch_size,256,1,1]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # loc_1\n",
    "        self.loc_1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=4*4,kernel_size=3,stride=1,padding=1), # [batch_size,16,38,38]\n",
    "        )\n",
    "        # conf_1\n",
    "        self.conf_1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=4*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(4*num_classes),38,38]\n",
    "        )\n",
    "        # loc_2\n",
    "        self.loc_2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024,out_channels=6*4,kernel_size=3,stride=1,padding=1), # [batch_size,24,19,19]\n",
    "        )\n",
    "        # conf_2\n",
    "        self.conf_2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1024,out_channels=6*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(6*num_classes),19,19]\n",
    "        ) \n",
    "        # loc_3\n",
    "        self.loc_3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=6*4,kernel_size=3,stride=1,padding=1), # [batch_size,24,10,10]\n",
    "        )\n",
    "        # conf_3\n",
    "        self.conf_3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=6*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(6*num_classes),10,10]\n",
    "        ) \n",
    "        # loc_4\n",
    "        self.loc_4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=6*4,kernel_size=3,stride=1,padding=1), # [batch_size,24,5,5]\n",
    "        )\n",
    "        # conf_4\n",
    "        self.conf_4=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=6*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(6*num_classes),5,5]\n",
    "        )       \n",
    "        # loc_5\n",
    "        self.loc_5=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=4*4,kernel_size=3,stride=1,padding=1), # [batch_size,16,3,3]\n",
    "        )\n",
    "        # conf_5\n",
    "        self.conf_5=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=4*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(4*num_classes),3,3]\n",
    "        )   \n",
    "        # loc_6\n",
    "        self.loc_6=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=4*4,kernel_size=3,stride=1,padding=1), # [batch_size,16,1,1]\n",
    "        )\n",
    "        # conf_6\n",
    "        self.conf_6=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=4*num_classes,kernel_size=3,stride=1,padding=1), # [batch_size,(4*num_classes),1,1]\n",
    "        )   \n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.block_1(x) # [batch_size,512,38,38] (Conv4_3+ReLU輸出)\n",
    "        n=self.l2norm(x)\n",
    "        loc1=self.loc_1(n).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf1=self.conf_1(n).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        x=self.block_2(x) # [batch_size,1024,19,19] (Conv7+ReLU輸出)\n",
    "        loc2=self.loc_2(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf2=self.conf_2(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        x=self.block_3(x) # [batch_size,512,10,10] (Conv8_2+ReLU輸出)\n",
    "        loc3=self.loc_3(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf3=self.conf_3(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        x=self.block_4(x) # [batch_size,256,5,5] (Conv9_2+ReLU輸出)\n",
    "        loc4=self.loc_4(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf4=self.conf_4(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        x=self.block_5(x) # [batch_size,256,3,3] (Conv10_2+ReLU輸出)\n",
    "        loc5=self.loc_5(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf5=self.conf_5(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        x=self.block_6(x) # [batch_size,256,1,1] (Conv11_2+ReLU輸出)\n",
    "        loc6=self.loc_6(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,4)\n",
    "        conf6=self.conf_6(x).permute(0,2,3,1).contiguous().view(batch_size,-1).view(batch_size,-1,num_classes)\n",
    "        loc=torch.cat((loc1,loc2,loc3,loc4,loc5,loc6),1) # [batch_size,8732,4]，import torch\n",
    "        conf=torch.cat((conf1,conf2,conf3,conf4,conf5,conf6),1) # [batch_size,8732,num_classes]，import torch\n",
    "        return loc,conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b2bb9c-63b6-44f4-81b3-86356bc7a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Norm(nn.Module):\n",
    "    def __init__(self,in_channels,scale):\n",
    "        super(L2Norm,self).__init__()\n",
    "        self.in_channels=in_channels\n",
    "        self.gamma=scale or None\n",
    "        self.eps=1e-10\n",
    "        self.weight=nn.Parameter(torch.Tensor(self.in_channels)) # from torch import nn，import torch\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        nn.init.constant_(self.weight,self.gamma) # from torch import nn \n",
    "    def forward(self,x):\n",
    "        norm=x.pow(2).sum(dim=1,keepdim=True).sqrt()+self.eps\n",
    "        x=torch.div(x,norm) # import torch\n",
    "        out=self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x)*x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476304e-bc9c-47f0-b6b2-24835b7e8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector=SSD().to(device)\n",
    "optimizer=torch.optim.Adam(detector.parameters(),lr=learning_rate) # import torch\n",
    "#optimizer=torch.optim.SGD(detecotr.parameters(),lr=learning_rate) # import torch\n",
    "scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size,0.1) # import torch\n",
    "num_anchor=anchor.size(0) # 8732，anchor為[8732,4]\n",
    "for i in range(1,epochs+1):\n",
    "    print('Running Epoch:'+str(i))\n",
    "    train_loss,train_batch,valid_loss,valid_batch=0,0,0,0\n",
    "    detector.train()\n",
    "    for img,bbox_,cls_ in train_loader: # 一個batch的img、bbox_、cls_，img：[batch_size,3,300,300]，bbox_：列表長度為batch_size，列表中每個元素為一個[該影像中的物件個數,4]的tensor，cls_：列表長度為batch_size，列表中每個元素為一個[該影像中的物件個數]的tensor\n",
    "        if img.size(0)!=batch_size: # 最後不足一個batch的訓練影像不進行訓練\n",
    "            break\n",
    "        img=img.to(device)\n",
    "        gHat=torch.Tensor(batch_size,num_anchor,4) # import torch\n",
    "        matched_class=torch.LongTensor(batch_size,num_anchor) # import torch\n",
    "        pos=list()\n",
    "        for j in range(batch_size):\n",
    "\n",
    "            # 找出每個錨框匹配的物件(以比較IoU為主，但若IoU值為該物件的最大IoU值，則直接指定對應該物件。若IoU為0，則隨便匹配物件，待之後用threshold去除)\n",
    "            bbox=bbox_[j].to(device) # [該影像中的物件個數,4]\n",
    "            cls=cls_[j].to(device) # [該影像中的物件個數]\n",
    "            num_objects=bbox.size(0) # 該影像中的物件個數，[1]。bbox：[該影像中的物件個數,4]\n",
    "            min_xy=torch.max(bbox[:,:2].unsqueeze(1).expand(num_objects,num_anchor,2),anchor[:,:2].broadcast_to(num_objects,num_anchor,2)) # 每個物件與8732個錨框比較取較大的xmin與ymin，[該影像中的物件個數,8732,2]，2表示較大的xmin與ymin，import torch\n",
    "            max_xy=torch.min(bbox[:,2:].unsqueeze(1).expand(num_objects,num_anchor,2),anchor[:,2:].broadcast_to(num_objects,num_anchor,2)) # 每個物件與8732個錨框比較取較小的xmax與ymax，[該影像中的物件個數,8732,2]，2表示較小xmax與ymax，import torch\n",
    "            side_length=(max_xy-min_xy).clamp(min=0) # 交集面積的邊長，[該影像中的物件個數,8732,2]\n",
    "            area_inter=side_length[:,:,0]*side_length[:,:,1] # 交集面積，[該影像中的物件個數,8732]\n",
    "            area_bbox=((bbox[:,2]-bbox[:,0])*(bbox[:,3]-bbox[:,1])).unsqueeze(1).expand(num_objects,num_anchor)\n",
    "            area_anchor=((anchor[:,2]-anchor[:,0])*(anchor[:,3]-anchor[:,1])).broadcast_to(num_objects,num_anchor)\n",
    "            IoU=area_inter/(area_bbox+area_anchor-area_inter) # IOU，[該影像中的物件個數,8732]\n",
    "            maxIoU_object,anchor_idx=torch.max(IoU,dim=1) # dim=1表示取每列的最大值。maxIoU_object為每個物件的最大IoU，[該影像中的物件個數]。anchor_idx為每個物件最大IoU的錨框編號，[該影像中的物件個數]。import torch\n",
    "            maxIoU_anchor,object_idx=torch.max(IoU,dim=0) # dim=0表示取每行的最大值。maxIoU_anchor為每個錨框的最大IoU值，[8732]。object_idx為每個錨框最大IoU的物件編號(非類別)(0,1,2,...)，[8732]。import torch\n",
    "            maxIoU_anchor.index_fill_(0,anchor_idx,2) # 修改maxIoU_anchor(每個錨框的最大IoU)，令每個物件最大IoU的錨框(即anchor_idx)的IoU為2\n",
    "            pos.append(maxIoU_anchor>=threshold) # True/False，利用threshold篩選出背景，令有匹配到物件的錨框為True，背景為False(大部分為False)，pos：列表長度為batch_size，列表中每個元素為[8732]的tensor\n",
    "            for k in range(num_objects): # k:0~(num_objects-1)\n",
    "                object_idx[anchor_idx[k]]=k # 將每個物件IoU=2的錨框所對應的物件指定為該物件\n",
    "            matched_bbox=bbox[object_idx] # 每個錨框匹配物件的邊界框，[8732,4]，[xmin ymin xmax ymax]\n",
    "            matched_class[j]=cls[object_idx] # 每個錨框匹配物件的類別(1,2, ...)，matched_class[0]：[8732]，matched_class：[batch_size,8732]\n",
    "            matched_class[j][maxIoU_anchor<threshold]=0 # 利用threshold決定那些錨框匹配的物件類別為背景(0)，matched_class表示每個錨框匹配物件的類別(0,1,2, ...)，[batch_size,8732]\n",
    "            gHat_cx=((matched_bbox[:,0]+matched_bbox[:,2])/2-(anchor[:,0]+anchor[:,2])/2)/((anchor[:,2]-anchor[:,0])*variances[0]) # [8732]\n",
    "            gHat_cy=((matched_bbox[:,1]+matched_bbox[:,3])/2-(anchor[:,1]+anchor[:,3])/2)/((anchor[:,3]-anchor[:,1])*variances[0]) # [8732]\n",
    "            gHat_w=torch.log((matched_bbox[:,2]-matched_bbox[:,0])/(anchor[:,2]-anchor[:,0]))/variances[1] # [8732]，import torch\n",
    "            gHat_h=torch.log((matched_bbox[:,3]-matched_bbox[:,1])/(anchor[:,3]-anchor[:,1]))/variances[1] # [8732]，import torch\n",
    "            gHat[j]=torch.stack((gHat_cx,gHat_cy,gHat_w,gHat_h),1) # gHat[0]：[8732,4]，gHat：[batch_size,8732,4]，import torch\n",
    "        pos=torch.stack(pos,0) # 將pos從list轉為tensor，pos：[batch_size,8732]，True/False，import torch\n",
    "        pred_loc,pred_conf=detector(img) # pred_loc：[batch_size,8732,4]，pred_conf：[batch_size,8732,num_classes]\n",
    "        gHat,matched_class=gHat.to(device),matched_class.to(device) # gHat：[batch_size,8732,4]，matched_class：[batch_size,8732]\n",
    "        num_pos=pos.sum(dim=1,keepdim=True) # 利用threshold篩選後有匹配到物件的錨框數量，[batch_size,1]\n",
    "        pos_expand=pos.unsqueeze(pos.dim()).expand_as(pred_loc) # 將[batch_size,8732]的pos擴增成[batch_size,8732,4]，Ture/False\n",
    "        pos_l=pred_loc[pos_expand].view(-1,4) # 取出有匹配到物件的錨框的pred_loc，[batch內有匹配到物件的錨框總數(即num_pos內值的加總),4]\n",
    "        pos_gHat=gHat[pos_expand].view(-1,4) # 取出有匹配到物件的錨框的gHat，[batch內有匹配到物件的錨框總數(即num_pos內值的加總),4]\n",
    "        L_loc=torch.nn.functional.smooth_l1_loss(pos_l,pos_gHat) # 計算一個batch內有匹配到物件的錨框的L_loc，[1]，import torch\n",
    "        batch_pred_conf=pred_conf.view(-1,num_classes) # 將pred_conf內的batch整合在一起，[batch_size*8732,num_classes]\n",
    "        crossEntropy=torch.logsumexp(batch_pred_conf,dim=1,keepdim=True)-batch_pred_conf.gather(1,matched_class.view(-1,1)) # 計算每個錨框匹配物件(包含背景)的負logsumexp，[batch_size*8732,1]，import torch\n",
    "            # matched_class.view(-1,1)：將matched_class(每個錨框匹配物件的類別(0,1,2, ...))內的batch整合在一起，[batch_size*8732,1]\n",
    "            # batch_pred_conf.gather(1,matched_class.view(-1,1))：根據matched_class.view(-1,1)(每個錨框匹配物件的類別(0,1,2, ...))取出該物件類別的預測置信值(pred_conf)\n",
    "        crossEntropy[pos.view(-1,1)]=0 # 利用threshold篩選後若錨框有匹配到物件，則令該錨框匹配物件的負logsumexp為0，[batch_size*8732,1]\n",
    "        crossEntropy=crossEntropy.view(batch_size,-1) # 將crossEntropy從[batch_size*8732,1]轉換成[batch_size,8732]        \n",
    "        _,background_idx=crossEntropy.sort(1,descending=True) # background_idx：[batch_size,8732]，將有匹配到物件的錨框的負logsumexp設為0後，依負logsumexp由大而小排列並取得錨框編號(如編號5即表示第5個錨框所匹配的背景的負logsumexp為最大)\n",
    "        _,idx_rank=background_idx.sort(1) # idx_rank：[batch_size,8732]，依crossEntropy由小而大排序，如4、1、3、2表示第1個錨框在負logsumexp中排第4(愈大表示負logsumexp愈小)，第2個錨框在負logsumexp中排第1\n",
    "        num_neg=torch.clamp(3*num_pos,max=pos.size(1)-num_pos) # num_neg：[batch_size,1]，定義每張影像的負樣本個數為正樣本個數的3倍，上限改為錨框個數-正樣本個數，import torch\n",
    "        neg=idx_rank<num_neg.expand_as(idx_rank) # neg：True/False，將負logsumexp最大的前num_neg個設為True，[batch_size,8732]\n",
    "        pos_pred_conf=pos.unsqueeze(2).expand_as(pred_conf) # True/False，將pred_conf中正樣本的部分令為True，其餘為False，[batch_size,8732,num_classes]，\n",
    "        neg_pred_conf=neg.unsqueeze(2).expand_as(pred_conf) # True/False，將pred_conf中負樣本的部分令為True，其餘為False，[batch_size,8732,num_classes]\n",
    "        input=pred_conf[(pos_pred_conf+neg_pred_conf).gt(0)].view(-1,num_classes) # 挑出正樣本與負樣本的pred_conf，[num_pos+num_neg,num_classes]\n",
    "        target=matched_class[(pos+neg).gt(0)] # 挑出正樣本與負樣本的類別(包含背景)，[num_pos+num_neg]\n",
    "        L_conf=torch.nn.functional.cross_entropy(input,target) # 計算一個batch內正樣本與負樣本的L_conf，[1]，import torch\n",
    "        N=num_pos.data.sum()\n",
    "        loss=(L_loc+L_conf)/N\n",
    "        train_loss+=loss.item()\n",
    "        train_batch+=1\n",
    "        optimizer.zero_grad() # 權重梯度歸零\n",
    "        loss.backward() # 計算每個權重的loss梯度\n",
    "        optimizer.step() # 權重更新\n",
    "    scheduler.step()\n",
    "    if train_batch!=0:\n",
    "        print('Training Loss='+str(train_loss/train_batch)) # 計算每一個epoch的平均訓練loss\n",
    "\n",
    "    detector.eval()\n",
    "    for img,bbox_,cls_ in valid_loader: # 一個batch的img、bbox_、cls_，img：[batch_size,3,300,300]，bbox_：列表長度為batch_size，列表中每個元素為一個[該影像中的物件個數,4]的tensor，cls_：列表長度為batch_size，列表中每個元素為一個[該影像中的物件個數]的tensor\n",
    "        if img.size(0)!=batch_size: # 最後不足一個batch的驗證影像不進行驗證\n",
    "            break \n",
    "        img=img.to(device)\n",
    "        gHat=torch.Tensor(batch_size,num_anchor,4) # import torch\n",
    "        matched_class=torch.LongTensor(batch_size,num_anchor) # import torch\n",
    "        pos=list()\n",
    "        for j in range(batch_size):\n",
    "\n",
    "            # 找出每個錨框匹配的物件(以比較IoU為主，但若IoU值為該物件的最大IoU值，則直接指定對應該物件。若IoU為0，則隨便匹配物件，待之後用threshold去除)\n",
    "            bbox=bbox_[j].to(device) # [該影像中的物件個數,4]\n",
    "            cls=cls_[j].to(device) # [該影像中的物件個數]\n",
    "            num_objects=bbox.size(0) # 該影像中的物件個數，[1]。bbox：[該影像中的物件個數,4]\n",
    "            min_xy=torch.max(bbox[:,:2].unsqueeze(1).expand(num_objects,num_anchor,2),anchor[:,:2].broadcast_to(num_objects,num_anchor,2)) # 每個物件與8732個錨框比較取較大的xmin與ymin，[該影像中的物件個數,8732,2]，2表示較大的xmin與ymin，import torch\n",
    "            max_xy=torch.min(bbox[:,2:].unsqueeze(1).expand(num_objects,num_anchor,2),anchor[:,2:].broadcast_to(num_objects,num_anchor,2)) # 每個物件與8732個錨框比較取較小的xmax與ymax，[該影像中的物件個數,8732,2]，2表示較小xmax與ymax，import torch\n",
    "            side_length=(max_xy-min_xy).clamp(min=0) # 交集面積的邊長，[該影像中的物件個數,8732,2]\n",
    "            area_inter=side_length[:,:,0]*side_length[:,:,1] # 交集面積，[該影像中的物件個數,8732]\n",
    "            area_bbox=((bbox[:,2]-bbox[:,0])*(bbox[:,3]-bbox[:,1])).unsqueeze(1).expand(num_objects,num_anchor)\n",
    "            area_anchor=((anchor[:,2]-anchor[:,0])*(anchor[:,3]-anchor[:,1])).broadcast_to(num_objects,num_anchor)\n",
    "            IoU=area_inter/(area_bbox+area_anchor-area_inter) # IOU，[該影像中的物件個數,8732]\n",
    "            maxIoU_object,anchor_idx=torch.max(IoU,dim=1) # dim=1表示取每列的最大值。maxIoU_object為每個物件的最大IoU，[該影像中的物件個數]。anchor_idx為每個物件最大IoU的錨框編號，[該影像中的物件個數]。import torch\n",
    "            maxIoU_anchor,object_idx=torch.max(IoU,dim=0) # dim=0表示取每行的最大值。maxIoU_anchor為每個錨框的最大IoU值，[8732]。object_idx為每個錨框最大IoU的物件編號(非類別)(0,1,2,...)，[8732]。import torch\n",
    "            maxIoU_anchor.index_fill_(0,anchor_idx,2) # 修改maxIoU_anchor(每個錨框的最大IoU)，令每個物件最大IoU的錨框(即anchor_idx)的IoU為2\n",
    "            pos.append(maxIoU_anchor>=threshold) # True/False，利用threshold篩選出背景，令有匹配到物件的錨框為True，背景為False(大部分為False)，pos：列表長度為batch_size，列表中每個元素為[8732]的tensor\n",
    "            for k in range(num_objects): # k:0~(num_objects-1)\n",
    "                object_idx[anchor_idx[k]]=k # 將每個物件IoU=2的錨框所對應的物件指定為該物件\n",
    "            matched_bbox=bbox[object_idx] # 每個錨框匹配物件的邊界框，[8732,4]，[xmin ymin xmax ymax]\n",
    "            matched_class[j]=cls[object_idx] # 每個錨框匹配物件的類別(1,2, ...)，matched_class[0]：[8732]，matched_class：[batch_size,8732]\n",
    "            matched_class[j][maxIoU_anchor<threshold]=0 # 利用threshold決定那些錨框匹配的物件類別為背景(0)，matched_class表示每個錨框匹配物件的類別(0,1,2, ...)，[batch_size,8732]\n",
    "            gHat_cx=((matched_bbox[:,0]+matched_bbox[:,2])/2-(anchor[:,0]+anchor[:,2])/2)/((anchor[:,2]-anchor[:,0])*variances[0]) # [8732]\n",
    "            gHat_cy=((matched_bbox[:,1]+matched_bbox[:,3])/2-(anchor[:,1]+anchor[:,3])/2)/((anchor[:,3]-anchor[:,1])*variances[0]) # [8732]\n",
    "            gHat_w=torch.log((matched_bbox[:,2]-matched_bbox[:,0])/(anchor[:,2]-anchor[:,0]))/variances[1] # [8732]，import torch\n",
    "            gHat_h=torch.log((matched_bbox[:,3]-matched_bbox[:,1])/(anchor[:,3]-anchor[:,1]))/variances[1] # [8732]，import torch\n",
    "            gHat[j]=torch.stack((gHat_cx,gHat_cy,gHat_w,gHat_h),1) # gHat[0]：[8732,4]，gHat：[batch_size,8732,4]，import torch\n",
    "        pos=torch.stack(pos,0) # 將pos從list轉為tensor，pos：[batch_size,8732]，True/False，import torch\n",
    "        pred_loc,pred_conf=detector(img) # pred_loc：[batch_size,8732,4]，pred_conf：[batch_size,8732,num_classes]\n",
    "        gHat,matched_class=gHat.to(device),matched_class.to(device) # gHat：[batch_size,8732,4]，matched_class：[batch_size,8732]\n",
    "        num_pos=pos.sum(dim=1,keepdim=True) # 利用threshold篩選後有匹配到物件的錨框數量，[batch_size,1]\n",
    "        pos_expand=pos.unsqueeze(pos.dim()).expand_as(pred_loc) # 將[batch_size,8732]的pos擴增成[batch_size,8732,4]，Ture/False\n",
    "        pos_l=pred_loc[pos_expand].view(-1,4) # 取出有匹配到物件的錨框的pred_loc，[batch內有匹配到物件的錨框總數(即num_pos內值的加總),4]\n",
    "        pos_gHat=gHat[pos_expand].view(-1,4) # 取出有匹配到物件的錨框的gHat，[batch內有匹配到物件的錨框總數(即num_pos內值的加總),4]\n",
    "        L_loc=torch.nn.functional.smooth_l1_loss(pos_l,pos_gHat) # 計算一個batch內有匹配到物件的錨框的L_loc，[1]，import torch\n",
    "        batch_pred_conf=pred_conf.view(-1,num_classes) # 將pred_conf內的batch整合在一起，[batch_size*8732,num_classes]\n",
    "        crossEntropy=torch.logsumexp(batch_pred_conf,dim=1,keepdim=True)-batch_pred_conf.gather(1,matched_class.view(-1,1)) # 計算每個錨框匹配物件(包含背景)的負logsumexp，[batch_size*8732,1]，import torch\n",
    "            # matched_class.view(-1,1)：將matched_class(每個錨框匹配物件的類別(0,1,2, ...))內的batch整合在一起，[batch_size*8732,1]\n",
    "            # batch_pred_conf.gather(1,matched_class.view(-1,1))：根據matched_class.view(-1,1)(每個錨框匹配物件的類別(0,1,2, ...))取出該物件類別的預測置信值(pred_conf)\n",
    "        crossEntropy[pos.view(-1,1)]=0 # 利用threshold篩選後若錨框有匹配到物件，則令該錨框匹配物件的負logsumexp為0，[batch_size*8732,1]\n",
    "        crossEntropy=crossEntropy.view(batch_size,-1) # 將crossEntropy從[batch_size*8732,1]轉換成[batch_size,8732]        \n",
    "        _,background_idx=crossEntropy.sort(1,descending=True) # background_idx：[batch_size,8732]，將有匹配到物件的錨框的負logsumexp設為0後，依負logsumexp由大而小排列並取得錨框編號(如編號5即表示第5個錨框所匹配的背景的負logsumexp為最大)\n",
    "        _,idx_rank=background_idx.sort(1) # idx_rank：[batch_size,8732]，依crossEntropy由小而大排序，如4、1、3、2表示第1個錨框在負logsumexp中排第4(愈大表示負logsumexp愈小)，第2個錨框在負logsumexp中排第1\n",
    "        num_neg=torch.clamp(3*num_pos,max=pos.size(1)-num_pos) # num_neg：[batch_size,1]，定義每張影像的負樣本個數為正樣本個數的3倍，上限改為錨框個數-正樣本個數，import torch\n",
    "        neg=idx_rank<num_neg.expand_as(idx_rank) # neg：True/False，將負logsumexp最大的前num_neg個設為True，[batch_size,8732]\n",
    "        pos_pred_conf=pos.unsqueeze(2).expand_as(pred_conf) # True/False，將pred_conf中正樣本的部分令為True，其餘為False，[batch_size,8732,num_classes]，\n",
    "        neg_pred_conf=neg.unsqueeze(2).expand_as(pred_conf) # True/False，將pred_conf中負樣本的部分令為True，其餘為False，[batch_size,8732,num_classes]\n",
    "        input=pred_conf[(pos_pred_conf+neg_pred_conf).gt(0)].view(-1,num_classes) # 挑出正樣本與負樣本的pred_conf，[num_pos+num_neg,num_classes]\n",
    "        target=matched_class[(pos+neg).gt(0)] # 挑出正樣本與負樣本的類別(包含背景)，[num_pos+num_neg]\n",
    "        L_conf=torch.nn.functional.cross_entropy(input,target) # 計算一個batch內正樣本與負樣本的L_conf，[1]，import torch\n",
    "        N=num_pos.data.sum()\n",
    "        loss=(L_loc+L_conf)/N\n",
    "        valid_loss+=loss.item()\n",
    "        valid_batch+=1\n",
    "    if valid_batch!=0:\n",
    "        print('Validation Loss='+str(valid_loss/valid_batch)) # 計算每一個epoch的平均驗證loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dccdad-21bd-4105-b42a-4d4fffdc54ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(detector.state_dict(),file_name) # import torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-final-report",
   "language": "python",
   "name": "ml-final-report"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
